
Model: "autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
img (InputLayer)             [(None, 28, 28, 1)]       0         
_________________________________________________________________
flatten_12 (Flatten)         (None, 784)               0         
_________________________________________________________________
dense_33 (Dense)             (None, 64)                50240     
_________________________________________________________________
dense_34 (Dense)             (None, 64)                4160      
_________________________________________________________________
dense_35 (Dense)             (None, 784)               50960     
_________________________________________________________________
reshape_12 (Reshape)         (None, 28, 28, 1)         0         
=================================================================
Total params: 105,360
Trainable params: 105,360
Non-trainable params: 0
_________________________________________________________________
epoch;loss;val_loss
0;0.024509510025382042;0.017025362700223923
1;0.01575404778122902;0.014947190880775452
2;0.014621016569435596;0.014284235425293446
3;0.014128807000815868;0.013954277150332928
4;0.013818087987601757;0.013769987039268017
5;0.013580338098108768;0.013405805453658104
6;0.0134234968572855;0.01326767262071371
7;0.01331161055713892;0.013228120282292366
8;0.01321527361869812;0.01310937199741602
9;0.013111692853271961;0.012999837286770344
10;0.013016999699175358;0.013073882088065147
11;0.012964817695319653;0.012957635335624218
12;0.012935475446283817;0.01283897366374731
13;0.01289494801312685;0.012516738846898079
14;0.01247138436883688;0.012377058155834675
15;0.012418493628501892;0.012394377961754799
16;0.012400506995618343;0.012339740060269833
17;0.01238721702247858;0.012464946135878563
18;0.012369371950626373;0.01233007200062275
19;0.012354983016848564;0.012302610091865063
0;0.11562143266201019;7410.12646484375
1;0.11200320720672607;7410.12646484375
2;0.11200277507305145;7410.12646484375
3;0.11200270056724548;7410.12646484375
4;0.11200267821550369;7410.12646484375
5;0.11200254410505295;7410.12646484375
6;0.11200258880853653;7410.12646484375
7;0.11200256645679474;7410.12646484375
8;0.11200253665447235;7410.12646484375
9;0.11200259625911713;7410.12646484375
10;0.11200257390737534;7410.12646484375
11;0.11200258135795593;7410.12646484375
12;0.11200257390737534;7410.12646484375
13;0.11200258135795593;7410.12646484375
14;0.11200260370969772;7410.12646484375
15;0.11200255900621414;7410.12646484375
16;0.11200258135795593;7410.12646484375
17;0.11200257390737534;7410.12646484375
18;0.11200257390737534;7410.12646484375
19;0.11200258880853653;7410.12646484375
0;0.11200262606143951;7410.1279296875
1;0.11200258135795593;7410.1279296875
2;0.11200252920389175;7410.1279296875
3;0.11200262606143951;7410.1279296875
4;0.11200262606143951;7410.1279296875
5;0.1120026558637619;7410.1279296875
6;0.1120026484131813;7410.1279296875
7;0.1120026558637619;7410.1279296875
8;0.11200254410505295;7410.1279296875
9;0.11200257390737534;7410.1279296875

